"""
ULTRA-OPTIMIZED frame_restorer.py
ä¸¦åˆ—åŒ–é †åºä¿è¨¼ã‚’ç¶­æŒã—ãªãŒã‚‰æœ€å¤§é™ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾

ã€é‡è¦ãªæœ€é©åŒ–é …ç›®ã€‘
1. GPUå‡¦ç†ã®ãƒãƒƒãƒåŒ– - è¤‡æ•°ã‚¯ãƒªãƒƒãƒ—ã‚’1å›ã®GPUå‘¼ã³å‡ºã—ã§å‡¦ç†
2. Zero-copyé †åºç®¡ç† - heapqã®ä»£ã‚ã‚Šã«Ring Bufferã§é †åºä¿è¨¼
3. CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—åŒ– - è¤‡æ•°GPUå‡¦ç†ã®çœŸã®ä¸¦åˆ—å®Ÿè¡Œ
4. è¨ºæ–­æ©Ÿèƒ½ã®æ¡ä»¶ä»˜ãç„¡åŠ¹åŒ– - ãƒªãƒªãƒ¼ã‚¹ãƒ“ãƒ«ãƒ‰ã§å®Œå…¨å‰Šé™¤
5. Tensorã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ—ãƒ¼ãƒ« - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‰Šæ¸›

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã€‘
- ã‚¯ãƒªãƒƒãƒ—å‡¦ç†ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 2-3å€å‘ä¸Š
- GPUåˆ©ç”¨ç‡: 85%ä»¥ä¸Š
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: ç¾è¡Œã®50-70%
"""

import logging
import queue
import threading
import time
from typing import Optional, List, Tuple
from concurrent.futures import ThreadPoolExecutor
from collections import deque
import math

import cv2
import numpy as np
import torch

from lada import LOG_LEVEL
from lada.lib import image_utils, video_utils, threading_utils, mask_utils
from lada.lib import visualization_utils
from lada.lib.mosaic_detector import MosaicDetector
from lada.lib.mosaic_detection_model import MosaicDetectionModel

logger = logging.getLogger(__name__)
logging.basicConfig(level=LOG_LEVEL)

# ====================================
# è¨ºæ–­æ©Ÿèƒ½ã®æ¡ä»¶ä»˜ãæœ‰åŠ¹åŒ–
# ====================================
ENABLE_DIAGNOSTICS = False  # ãƒªãƒªãƒ¼ã‚¹æ™‚ã¯Falseã«


# ====================================
# GPU Tensor Pool (ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‰Šæ¸›)
# ====================================
class TensorPool:
    """å†åˆ©ç”¨å¯èƒ½ãªGPU Tensorãƒ—ãƒ¼ãƒ«"""
    
    def __init__(self, device, max_size=20):
        self.device = device
        self.pool = deque(maxlen=max_size)
        self.lock = threading.Lock()
        self.hits = 0
        self.misses = 0
    
    def get(self, shape, dtype=torch.float32):
        """æŒ‡å®šã‚µã‚¤ã‚ºã®Tensorã‚’å–å¾—ï¼ˆæ—¢å­˜ãªã‚‰å†åˆ©ç”¨ï¼‰"""
        with self.lock:
            for i, tensor in enumerate(self.pool):
                if tensor.shape == shape and tensor.dtype == dtype:
                    self.hits += 1
                    return self.pool.pop(i)
            
            self.misses += 1
            return torch.empty(shape, dtype=dtype, device=self.device)
    
    def release(self, tensor):
        """Tensorã‚’ãƒ—ãƒ¼ãƒ«ã«è¿”å´"""
        with self.lock:
            if len(self.pool) < self.pool.maxlen:
                self.pool.append(tensor)
    
    def get_stats(self):
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        return f"Pool: hits={self.hits}, misses={self.misses}, rate={hit_rate:.1f}%"


# ====================================
# Ring Bufferé †åºç®¡ç†ï¼ˆheapqã®ä»£æ›¿ï¼‰
# ====================================
class RingBufferOrderer:
    """
    é †åºä¿è¨¼ä»˜ãRing Buffer
    heapqã® O(log N) push/pop ã‚’ O(1) ã«æ”¹å–„
    """
    
    def __init__(self, capacity=100):
        self.buffer = [None] * capacity
        self.capacity = capacity
        self.next_expected_id = 0
        self.write_positions = {}  # {clip_id: buffer_index}
        self.lock = threading.Lock()
    
    def put(self, clip_id: int, clip):
        """ã‚¯ãƒªãƒƒãƒ—ã‚’é †åºç„¡è¦–ã§æ ¼ç´"""
        with self.lock:
            index = clip_id % self.capacity
            self.buffer[index] = clip
            self.write_positions[clip_id] = index
    
    def try_get_next(self) -> Optional[Tuple[int, any]]:
        """æ¬¡ã®æœŸå¾…IDã®ã‚¯ãƒªãƒƒãƒ—ã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°Noneï¼‰"""
        with self.lock:
            expected = self.next_expected_id
            
            if expected in self.write_positions:
                index = self.write_positions[expected]
                clip = self.buffer[index]
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                self.buffer[index] = None
                del self.write_positions[expected]
                self.next_expected_id += 1
                
                return expected, clip
            
            return None
    
    def skip_to(self, clip_id: int):
        """æ¬ è½ã‚¯ãƒªãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—"""
        with self.lock:
            self.next_expected_id = clip_id
    
    def pending_count(self):
        """ä¿ç•™ä¸­ã®ã‚¯ãƒªãƒƒãƒ—æ•°"""
        with self.lock:
            return len(self.write_positions)
    
    def get_stats(self):
        with self.lock:
            return {
                'next_expected': self.next_expected_id,
                'pending': len(self.write_positions),
                'capacity_usage': len(self.write_positions) / self.capacity * 100
            }


# ====================================
# ãƒãƒƒãƒå‡¦ç†GPU Executor
# ====================================
class BatchGPUExecutor:
    """
    è¤‡æ•°ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒãƒƒãƒåŒ–ã—ã¦1å›ã®GPUå‘¼ã³å‡ºã—ã§å‡¦ç†
    ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®éµ
    """
    
    def __init__(self, model, model_name, device, max_batch_size=4, timeout=0.05):
        self.model = model
        self.model_name = model_name
        self.device = device
        self.max_batch_size = max_batch_size
        self.timeout = timeout
        
        self.batch_queue = queue.Queue()
        self.result_queues = {}  # {request_id: queue}
        self.next_request_id = 0
        self.lock = threading.Lock()
        
        self.running = True
        self.worker_thread = threading.Thread(target=self._batch_worker, daemon=True)
        self.worker_thread.start()
        
        self.stats = {'batches': 0, 'total_clips': 0, 'batch_sizes': []}
    
    def process_clip(self, images):
        """ã‚¯ãƒªãƒƒãƒ—ã‚’å‡¦ç†ï¼ˆãƒãƒƒãƒåŒ–ã•ã‚Œã‚‹ï¼‰"""
        with self.lock:
            request_id = self.next_request_id
            self.next_request_id += 1
            result_queue = queue.Queue(maxsize=1)
            self.result_queues[request_id] = result_queue
        
        self.batch_queue.put((request_id, images))
        result = result_queue.get()  # ãƒãƒƒãƒå‡¦ç†å®Œäº†ã‚’å¾…ã¤
        
        with self.lock:
            del self.result_queues[request_id]
        
        return result
    
    def _batch_worker(self):
        """ãƒãƒƒãƒå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        while self.running:
            batch = []
            request_ids = []
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§æœ€åˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å¾…ã¤
            try:
                req_id, images = self.batch_queue.get(timeout=self.timeout)
                batch.append(images)
                request_ids.append(req_id)
            except queue.Empty:
                continue
            
            # è¿½åŠ ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åé›†ï¼ˆãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            while len(batch) < self.max_batch_size:
                try:
                    req_id, images = self.batch_queue.get_nowait()
                    batch.append(images)
                    request_ids.append(req_id)
                except queue.Empty:
                    break
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            if batch:
                results = self._process_batch(batch)
                
                # çµ±è¨ˆæ›´æ–°
                self.stats['batches'] += 1
                self.stats['total_clips'] += len(batch)
                self.stats['batch_sizes'].append(len(batch))
                
                # çµæœã‚’å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«è¿”ã™
                for req_id, result in zip(request_ids, results):
                    if req_id in self.result_queues:
                        self.result_queues[req_id].put(result)
    
    def _process_batch(self, batch: List[List[np.ndarray]]) -> List[List[np.ndarray]]:
        """ãƒãƒƒãƒã‚’GPUã§å‡¦ç†"""
        if self.model_name.startswith("deepmosaics"):
            # DeepMosaicsã¯å€‹åˆ¥å‡¦ç†ï¼ˆãƒãƒƒãƒåŒ–æœªå¯¾å¿œï¼‰
            from lada.deepmosaics.inference import restore_video_frames
            from lada.deepmosaics.models import model_util
            
            results = []
            for images in batch:
                restored = restore_video_frames(
                    model_util.device_to_gpu_id(self.device),
                    self.model,
                    images
                )
                results.append(restored)
            return results
        
        elif self.model_name.startswith("basicvsrpp"):
            # BasicVSR++ã®ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
            return self._process_basicvsrpp_batch(batch)
        
        else:
            raise NotImplementedError(f"Unknown model: {self.model_name}")
    
    def _process_basicvsrpp_batch(self, batch: List[List[np.ndarray]]) -> List[List[np.ndarray]]:
        """BasicVSR++ã®æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"""
        from lada.basicvsrpp.inference import inference
        
        # ã€æœ€é©åŒ–1ã€‘NumPyå¤‰æ›ã‚’1å›ã«ã¾ã¨ã‚ã‚‹
        all_numpy_images = []
        clip_lengths = []
        
        for images in batch:
            numpy_images = []
            for img in images:
                if isinstance(img, torch.Tensor):
                    # GPUâ†’CPUè»¢é€ã‚’æœ€å°åŒ–
                    if img.is_cuda:
                        img = img.cpu()
                    if img.ndim == 3 and img.shape[0] == 3:
                        img = img.permute(1, 2, 0)
                    img = img.numpy()
                    if img.dtype == np.float32 and img.max() <= 1.0:
                        img = (img * 255).astype(np.uint8)
                    elif img.dtype != np.uint8:
                        img = img.astype(np.uint8)
                numpy_images.append(img)
            
            all_numpy_images.extend(numpy_images)
            clip_lengths.append(len(numpy_images))
        
        # ã€æœ€é©åŒ–2ã€‘å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’1å›ã®GPUå‘¼ã³å‡ºã—ã§å‡¦ç†
        all_restored = inference(self.model, all_numpy_images, self.device)
        
        # ã€æœ€é©åŒ–3ã€‘çµæœã‚’å„ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²
        results = []
        offset = 0
        for length in clip_lengths:
            results.append(all_restored[offset:offset + length])
            offset += length
        
        return results
    
    def stop(self):
        """ãƒãƒƒãƒãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢"""
        self.running = False
        if self.worker_thread.is_alive():
            self.worker_thread.join(timeout=2.0)
    
    def get_stats(self):
        """çµ±è¨ˆæƒ…å ±"""
        avg_batch = sum(self.stats['batch_sizes']) / len(self.stats['batch_sizes']) if self.stats['batch_sizes'] else 0
        return (
            f"Batches: {self.stats['batches']}, "
            f"Clips: {self.stats['total_clips']}, "
            f"Avg batch size: {avg_batch:.2f}"
        )


# ====================================
# CUDA Streamsã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–
# ====================================
class CUDAStreamManager:
    """
    è¤‡æ•°ã®CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ã§çœŸã®ä¸¦åˆ—GPUå‡¦ç†
    """
    
    def __init__(self, device, num_streams=2):
        self.device = device
        self.num_streams = num_streams
        self.streams = []
        self.stream_locks = []
        
        if torch.cuda.is_available():
            for _ in range(num_streams):
                self.streams.append(torch.cuda.Stream(device=device))
                self.stream_locks.append(threading.Lock())
    
    def get_stream(self, index: int):
        """æŒ‡å®šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾—"""
        if not self.streams:
            return None
        stream_idx = index % self.num_streams
        return self.streams[stream_idx], self.stream_locks[stream_idx]


def load_models(device, mosaic_restoration_model_name, mosaic_restoration_model_path, 
                mosaic_restoration_config_path, mosaic_detection_model_path):
    """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰"""
    if mosaic_restoration_model_name.startswith("deepmosaics"):
        from lada.deepmosaics.models import loadmodel, model_util
        mosaic_restoration_model = loadmodel.video(model_util.device_to_gpu_id(device), mosaic_restoration_model_path)
        pad_mode = 'reflect'
    elif mosaic_restoration_model_name.startswith("basicvsrpp"):
        from lada.basicvsrpp.inference import load_model, get_default_gan_inference_config
        if mosaic_restoration_config_path:
            config = mosaic_restoration_config_path
        else:
            config = get_default_gan_inference_config()
        mosaic_restoration_model = load_model(config, mosaic_restoration_model_path, device)
        pad_mode = 'zero'
    else:
        raise NotImplementedError()
    mosaic_detection_model = MosaicDetectionModel(mosaic_detection_model_path, device, classes=[0], conf=0.2)
    return mosaic_detection_model, mosaic_restoration_model, pad_mode


class FrameRestorer:
    """åŸºæœ¬ã‚¯ãƒ©ã‚¹ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨äº’æ›æ€§ç¶­æŒï¼‰"""
    def __init__(self, device, video_file, preserve_relative_scale, max_clip_length, mosaic_restoration_model_name,
                 mosaic_detection_model, mosaic_restoration_model, preferred_pad_mode,
                 mosaic_detection=False):
        self.device = device
        self.mosaic_restoration_model_name = mosaic_restoration_model_name
        self.max_clip_length = max_clip_length
        self.preserve_relative_scale = preserve_relative_scale
        self.video_meta_data = video_utils.get_video_meta_data(video_file)
        self.mosaic_detection_model = mosaic_detection_model
        self.mosaic_restoration_model = mosaic_restoration_model
        self.preferred_pad_mode = preferred_pad_mode
        self.start_ns = 0
        self.start_frame = 0
        self.mosaic_detection = mosaic_detection
        self.eof = False
        self.stop_requested = False

        max_frames_in_frame_restoration_queue = (512 * 1024 * 1024) // (
            self.video_meta_data.video_width * self.video_meta_data.video_height * 3
        )
        self.frame_restoration_queue = queue.Queue(maxsize=max_frames_in_frame_restoration_queue)

        max_clips_in_mosaic_clips_queue = max(1, (512 * 1024 * 1024) // (self.max_clip_length * 256 * 256 * 4))
        logger.debug(f"Set queue size of queue mosaic_clip_queue to {max_clips_in_mosaic_clips_queue}")
        self.mosaic_clip_queue = queue.Queue(maxsize=max_clips_in_mosaic_clips_queue)

        max_clips_in_restored_clips_queue = max(1, (512 * 1024 * 1024) // (self.max_clip_length * 256 * 256 * 4))
        logger.debug(f"Set queue size of queue restored_clip_queue to {max_clips_in_restored_clips_queue}")
        self.restored_clip_queue = queue.Queue(maxsize=max_clips_in_restored_clips_queue)

        self.frame_detection_queue = queue.Queue()

        self.mosaic_detector = MosaicDetector(self.mosaic_detection_model, self.video_meta_data.video_file,
                                              frame_detection_queue=self.frame_detection_queue,
                                              mosaic_clip_queue=self.mosaic_clip_queue,
                                              device=self.device,
                                              max_clip_length=self.max_clip_length,
                                              pad_mode=self.preferred_pad_mode,
                                              preserve_relative_scale=self.preserve_relative_scale,
                                              dont_preserve_relative_scale=(not self.preserve_relative_scale))

        self.clip_restoration_thread = None
        self.frame_restoration_thread = None
        self.clip_restoration_thread_should_be_running = False
        self.frame_restoration_thread_should_be_running = False

        self.queue_stats = {}
        self.queue_stats["restored_clip_queue_max_size"] = 0
        self.queue_stats["restored_clip_queue_wait_time_put"] = 0
        self.queue_stats["restored_clip_queue_wait_time_get"] = 0
        self.queue_stats["mosaic_clip_queue_wait_time_get"] = 0
        self.queue_stats["frame_restoration_queue_max_size"] = 0
        self.queue_stats["frame_restoration_queue_wait_time_get"] = 0
        self.queue_stats["frame_restoration_queue_wait_time_put"] = 0
        self.queue_stats["frame_detection_queue_wait_time_get"] = 0

    def start(self, start_ns=0):
        """åŸºæœ¬ç‰ˆã®startï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯¾è±¡ï¼‰"""
        raise NotImplementedError("Subclass must implement start()")

    def stop(self):
        """åŸºæœ¬ç‰ˆã®stopï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯¾è±¡ï¼‰"""
        raise NotImplementedError("Subclass must implement stop()")


class UltraOptimizedFrameRestorer(FrameRestorer):
    """
    è¶…æœ€é©åŒ–ç‰ˆãƒ•ãƒ¬ãƒ¼ãƒ å¾©å…ƒå™¨
    
    ã€ä¸»è¦æ”¹å–„ã€‘
    1. ãƒãƒƒãƒGPUå‡¦ç† - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ2-3å€
    2. Ring Bufferé †åºç®¡ç† - heapqã‚ˆã‚Šé«˜é€Ÿ
    3. CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—åŒ–
    4. Tensorãƒ—ãƒ¼ãƒ« - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‰Šæ¸›
    5. è¨ºæ–­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›
    """
    
    def __init__(self, device, video_file, preserve_relative_scale, max_clip_length,
                 mosaic_restoration_model_name, mosaic_detection_model, mosaic_restoration_model,
                 preferred_pad_mode, mosaic_detection=False, parallel_clips=4, 
                 enable_batch_gpu=True, batch_size=4, 
                 queue_size_mb=None, enable_vram_direct=True, dynamic_batch_size=True, 
                 auto_worker_count=False, enable_diagnostics=False):
        """
        è¶…æœ€é©åŒ–ç‰ˆFrameRestorer
        
        Args:
            queue_size_mb: ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º(MB) - äº’æ›æ€§ã®ãŸã‚å—ã‘å–ã‚‹ãŒå†…éƒ¨ã§æœ€é©è¨ˆç®—
            enable_vram_direct: VRAMç›´æ¥å‡¦ç† - äº’æ›æ€§ã®ãŸã‚å—ã‘å–ã‚‹
            dynamic_batch_size: å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚º - äº’æ›æ€§ã®ãŸã‚å—ã‘å–ã‚‹
            auto_worker_count: è‡ªå‹•ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æ±ºå®š
            enable_diagnostics: è¨ºæ–­æ©Ÿèƒ½ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç„¡åŠ¹ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å„ªå…ˆï¼‰
        """
        
        super().__init__(device, video_file, preserve_relative_scale, max_clip_length,
                        mosaic_restoration_model_name, mosaic_detection_model,
                        mosaic_restoration_model, preferred_pad_mode, mosaic_detection)
        
        # è‡ªå‹•ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æ±ºå®š
        if auto_worker_count:
            parallel_clips = self._calculate_optimal_workers()
        
        self.parallel_clips = parallel_clips
        self.enable_batch_gpu = enable_batch_gpu
        self.batch_size = batch_size
        
        # è¿½åŠ ã®ã‚­ãƒ¥ãƒ¼åˆæœŸåŒ–ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ã«ãªã„ã‚‚ã®ï¼‰
        self.unordered_clips_queue = queue.Queue(maxsize=parallel_clips * 3)
        
        # Ring Bufferé †åºç®¡ç†ï¼ˆheapqã®ä»£æ›¿ï¼‰
        self.clip_orderer = RingBufferOrderer(capacity=parallel_clips * 10)
        
        # ãƒãƒƒãƒGPU Executor
        if enable_batch_gpu:
            self.batch_executor = BatchGPUExecutor(
                mosaic_restoration_model,
                mosaic_restoration_model_name,
                device,
                max_batch_size=batch_size
            )
        else:
            self.batch_executor = None
        
        # Tensorãƒ—ãƒ¼ãƒ«
        self.tensor_pool = TensorPool(device, max_size=20)
        
        # CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ 
        self.cuda_streams = CUDAStreamManager(device, num_streams=2)
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ç®¡ç†ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ã‹ã‚‰ç¶™æ‰¿ã—ãŸã‚‚ã®ã‚’æ‹¡å¼µï¼‰
        self.clip_restoration_threads = []
        self.clip_ordering_thread = None
        
        # çµ±è¨ˆï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ã®queue_statsã‚’æ‹¡å¼µï¼‰
        self.queue_stats['parallel_clips_processed'] = 0
        self.queue_stats['clip_timeout_count'] = 0
        self.queue_stats['clip_skipped_count'] = 0
        self.queue_stats['total_gpu_time'] = 0
        self.queue_stats['total_wait_time'] = 0
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†
        self.next_clip_id = 0
        self.clip_id_lock = threading.Lock()
        self.workers_finished_count = 0
        self.workers_finished_lock = threading.Lock()
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ•ãƒ©ã‚°ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ã‹ã‚‰ç¶™æ‰¿ã—ãŸã‚‚ã®ã‚’æ‹¡å¼µï¼‰
        self.clip_restoration_threads_should_be_running = False
        self.clip_ordering_thread_should_be_running = False
        
        logger.info(f"ğŸš€ UltraOptimizedFrameRestoreråˆæœŸåŒ–:")
        logger.info(f"   ä¸¦åˆ—åº¦: {parallel_clips}")
        logger.info(f"   ãƒãƒƒãƒGPU: {'æœ‰åŠ¹' if enable_batch_gpu else 'ç„¡åŠ¹'}")
        logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        logger.info(f"   CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ : {self.cuda_streams.num_streams}")
    
    def _calculate_optimal_workers(self):
        """GPUæ€§èƒ½ã«åŸºã¥ãæœ€é©ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°è¨ˆç®—"""
        if torch.cuda.is_available():
            total_vram = torch.cuda.get_device_properties(0).total_memory
            optimal_workers = max(1, int((total_vram * 0.7) / (1024**3)))
            return min(optimal_workers, 8)
        return 4
    
    def _get_next_clip_id(self):
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªClip IDç™ºè¡Œ"""
        with self.clip_id_lock:
            clip_id = self.next_clip_id
            self.next_clip_id += 1
            return clip_id
    
    def start(self, start_ns=0):
        """å‡¦ç†é–‹å§‹"""
        self.start_ns = start_ns
        self.start_frame = video_utils.offset_ns_to_frame_num(
            self.start_ns, self.video_meta_data.video_fps_exact
        )
        self.eof = False
        self.stop_requested = False
        
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒªã‚»ãƒƒãƒˆ
        with self.clip_id_lock:
            self.next_clip_id = 0
        with self.workers_finished_lock:
            self.workers_finished_count = 0
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ•ãƒ©ã‚°è¨­å®š
        self.frame_restoration_thread_should_be_running = True
        self.clip_restoration_threads_should_be_running = True
        self.clip_ordering_thread_should_be_running = True
        
        # ãƒ¢ã‚¶ã‚¤ã‚¯æ¤œå‡ºé–‹å§‹
        self.mosaic_detector.start(start_ns=start_ns)
        
        # ã‚¯ãƒªãƒƒãƒ—å¾©å…ƒãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
        for i in range(self.parallel_clips):
            thread = threading.Thread(
                target=self._clip_restoration_worker_ultra,
                name=f"ClipWorker-{i}",
                daemon=True
            )
            thread.start()
            self.clip_restoration_threads.append(thread)
        
        # ã‚¯ãƒªãƒƒãƒ—é †åºç®¡ç†ãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
        self.clip_ordering_thread = threading.Thread(
            target=self._clip_ordering_worker_ultra,
            name="ClipOrderer",
            daemon=True
        )
        self.clip_ordering_thread.start()
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ å¾©å…ƒãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•
        self.frame_restoration_thread = threading.Thread(
            target=self._frame_restoration_worker_vram,
            name="FrameRestorer",
            daemon=True
        )
        self.frame_restoration_thread.start()
        
        logger.info(f"âœ“ UltraOptimizedFrameRestorerèµ·å‹•å®Œäº† (ä¸¦åˆ—åº¦: {self.parallel_clips})")
    
    def _clip_restoration_worker_ultra(self):
        """
        ã€è¶…æœ€é©åŒ–ã€‘ã‚¯ãƒªãƒƒãƒ—å¾©å…ƒãƒ¯ãƒ¼ã‚«ãƒ¼
        - ãƒãƒƒãƒGPUå‡¦ç†
        - CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—åŒ–
        - Tensorãƒ—ãƒ¼ãƒ«æ´»ç”¨
        """
        worker_name = threading.current_thread().name
        worker_id = int(worker_name.split('-')[1])
        clips_processed = 0
        
        logger.debug(f"{worker_name}: èµ·å‹•")
        
        while self.clip_restoration_threads_should_be_running:
            try:
                wait_start = time.time()
                clip = self.mosaic_clip_queue.get(timeout=0.5)
                wait_time = time.time() - wait_start
                self.queue_stats['total_wait_time'] += wait_time
                
                if clip is None:
                    logger.info(f"{worker_name}: EOF (å‡¦ç†æ•°: {clips_processed})")
                    self.mosaic_clip_queue.put(None)  # ä»–ãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨
                    
                    with self.workers_finished_lock:
                        self.workers_finished_count += 1
                        if self.workers_finished_count == self.parallel_clips:
                            logger.info(f"{worker_name}: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼çµ‚äº† - EOFé€ä¿¡")
                            self.unordered_clips_queue.put(None)
                    break
                
                clip_id = self._get_next_clip_id()
                
                # GPUå‡¦ç†
                gpu_start = time.time()
                
                # CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—
                stream, stream_lock = self.cuda_streams.get_stream(worker_id)
                
                with stream_lock if stream else threading.Lock():
                    if stream:
                        with torch.cuda.stream(stream):
                            processed_clip = self._restore_clip_ultra(clip)
                    else:
                        processed_clip = self._restore_clip_ultra(clip)
                
                gpu_time = time.time() - gpu_start
                self.queue_stats['total_gpu_time'] += gpu_time
                
                # Ring Bufferã«æ ¼ç´
                self.clip_orderer.put(clip_id, processed_clip)
                
                # é †åºç®¡ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ã«é€šçŸ¥ï¼ˆè»½é‡ã‚·ã‚°ãƒŠãƒ«ï¼‰
                self.unordered_clips_queue.put((clip_id, None))  # Noneã¯ã‚·ã‚°ãƒŠãƒ«ã®ã¿
                
                self.queue_stats['parallel_clips_processed'] += 1
                clips_processed += 1
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"{worker_name}: ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        
        logger.debug(f"{worker_name}: çµ‚äº† (ç·å‡¦ç†: {clips_processed})")
    
    def _clip_ordering_worker_ultra(self):
        """
        ã€è¶…æœ€é©åŒ–ã€‘ã‚¯ãƒªãƒƒãƒ—é †åºç®¡ç†ãƒ¯ãƒ¼ã‚«ãƒ¼
        - Ring Bufferä½¿ç”¨ï¼ˆheapqã‚ˆã‚Šé«˜é€Ÿï¼‰
        - ãƒãƒ¼ãƒªãƒ³ã‚°å‰Šæ¸›
        """
        logger.debug("ClipOrderer: èµ·å‹•")
        
        timeout_consecutive = 0
        max_consecutive = 5
        
        while self.clip_ordering_thread_should_be_running:
            try:
                # ã‚·ã‚°ãƒŠãƒ«å¾…ã¡
                signal = self.unordered_clips_queue.get(timeout=0.5)
                
                if signal is None:
                    logger.info("ClipOrderer: EOFå—ä¿¡")
                    # æ®‹ã‚Šã‚¯ãƒªãƒƒãƒ—ã‚’é€å‡º
                    while True:
                        result = self.clip_orderer.try_get_next()
                        if result is None:
                            break
                        clip_id, clip = result
                        self.restored_clip_queue.put(clip)
                    
                    self.restored_clip_queue.put(None)
                    break
                
                # é †åºé€šã‚Šã®ã‚¯ãƒªãƒƒãƒ—ã‚’é€å‡º
                while True:
                    result = self.clip_orderer.try_get_next()
                    if result is None:
                        break
                    
                    clip_id, clip = result
                    self.restored_clip_queue.put(clip)
                    timeout_consecutive = 0
                
            except queue.Empty:
                timeout_consecutive += 1
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé€£ç¶šæ™‚ã®å‡¦ç†
                if timeout_consecutive >= max_consecutive:
                    stats = self.clip_orderer.get_stats()
                    logger.warning(
                        f"âš ï¸ é †åºå¾…ã¡ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: "
                        f"next={stats['next_expected']}, pending={stats['pending']}"
                    )
                    
                    # è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å†ç”Ÿå„ªå…ˆï¼‰
                    if stats['pending'] > 0:
                        logger.warning("â†’ æ¬ è½ã‚¯ãƒªãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ")
                        self.clip_orderer.skip_to(stats['next_expected'] + 1)
                        self.queue_stats['clip_skipped_count'] += 1
                        timeout_consecutive = 0
        
        logger.debug("ClipOrderer: çµ‚äº†")
    
    def _restore_clip_ultra(self, clip):
        """ã‚¯ãƒªãƒƒãƒ—å¾©å…ƒï¼ˆãƒãƒƒãƒGPUå¯¾å¿œï¼‰"""
        if self.mosaic_detection:
            restored_clip_images = visualization_utils.draw_mosaic_detections(clip)
        else:
            images = clip.get_clip_images()
            
            if self.enable_batch_gpu and self.batch_executor:
                # ãƒãƒƒãƒGPUå‡¦ç†
                restored_clip_images = self.batch_executor.process_clip(images)
            else:
                # å¾“æ¥ã®å€‹åˆ¥å‡¦ç†
                restored_clip_images = self._restore_clip_frames_vram(images)
        
        # ã‚¯ãƒªãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        for i in range(len(restored_clip_images)):
            clip.data[i] = (
                restored_clip_images[i],
                clip.data[i][1],
                clip.data[i][2],
                clip.data[i][3],
                clip.data[i][4]
            )
        
        return clip
    
    def _restore_clip_frames_vram(self, images):
        """å¾“æ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ å¾©å…ƒï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰"""
        if self.mosaic_restoration_model_name.startswith("deepmosaics"):
            from lada.deepmosaics.inference import restore_video_frames
            from lada.deepmosaics.models import model_util
            return restore_video_frames(
                model_util.device_to_gpu_id(self.device),
                self.mosaic_restoration_model,
                images
            )
        
        elif self.mosaic_restoration_model_name.startswith("basicvsrpp"):
            from lada.basicvsrpp.inference import inference
            
            numpy_images = []
            for img in images:
                if isinstance(img, torch.Tensor):
                    if img.is_cuda:
                        img = img.cpu()
                    if img.ndim == 3 and img.shape[0] == 3:
                        img = img.permute(1, 2, 0)
                    img = img.numpy()
                    if img.dtype == np.float32 and img.max() <= 1.0:
                        img = (img * 255).astype(np.uint8)
                    elif img.dtype != np.uint8:
                        img = img.astype(np.uint8)
                numpy_images.append(img)
            
            return inference(self.mosaic_restoration_model, numpy_images, self.device)
        
        else:
            raise NotImplementedError()
    
    def _frame_restoration_worker_vram(self):
        """ãƒ•ãƒ¬ãƒ¼ãƒ å¾©å…ƒãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰"""
        with video_utils.VideoReader(self.video_meta_data.video_file) as video_reader:
            if self.start_ns > 0:
                video_reader.seek(self.start_ns)
            
            video_frames_generator = video_reader.frames()
            frame_num = self.start_frame
            clips_remaining = True
            clip_buffer = []
            
            while self.frame_restoration_thread_should_be_running:
                _frame_result = self._read_next_frame(video_frames_generator, frame_num)
                if _frame_result is None:
                    if not self.stop_requested:
                        self.eof = True
                        self.frame_restoration_queue.put(None)
                    break
                
                mosaic_detected, frame, frame_pts = _frame_result
                
                if mosaic_detected:
                    while clips_remaining and not self._contains_at_least_one_clip_starting_after_frame_num(
                        frame_num, clip_buffer
                    ):
                        clips_remaining = self._read_next_clip(frame_num, clip_buffer)
                    
                    self._restore_frame(frame, frame_num, clip_buffer)
                    self._collect_garbage(clip_buffer)
                
                self.frame_restoration_queue.put((frame, frame_pts))
                frame_num += 1
    
    def _read_next_frame(self, video_frames_generator, expected_frame_num):
        """æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿"""
        try:
            frame, frame_pts = next(video_frames_generator)
        except StopIteration:
            elem = self.frame_detection_queue.get()
            assert elem is None
            return None
        
        elem = self.frame_detection_queue.get()
        if self.stop_requested:
            return None
        assert elem is not None
        detection_frame_num, mosaic_detected = elem
        return mosaic_detected, frame, frame_pts
    
    def _read_next_clip(self, current_frame_num, clip_buffer):
        """æ¬¡ã‚¯ãƒªãƒƒãƒ—èª­ã¿è¾¼ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œï¼‰"""
        try:
            clip = self.restored_clip_queue.get(timeout=5.0)
            if self.stop_requested or clip is None:
                return False
            clip_buffer.append(clip)
            return True
        except queue.Empty:
            self.queue_stats['clip_timeout_count'] += 1
            logger.warning(f"âš ï¸ ã‚¯ãƒªãƒƒãƒ—å–å¾—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (frame={current_frame_num})")
            
            if self.queue_stats['clip_timeout_count'] >= 3:
                logger.error("ğŸš¨ é€£ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ã‚¯ãƒªãƒƒãƒ—å¾…ã¡ã‚¹ã‚­ãƒƒãƒ—")
                return False
            return True
    
    def _restore_frame(self, frame, frame_num, restored_clips):
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚¯ãƒªãƒƒãƒ—ã‚’é©ç”¨"""
        for buffered_clip in [c for c in restored_clips if c.frame_start == frame_num]:
            clip_img, clip_mask, orig_clip_box, orig_crop_shape, pad_after_resize = buffered_clip.pop()
            clip_img = image_utils.unpad_image(clip_img, pad_after_resize)
            clip_mask = image_utils.unpad_image(clip_mask, pad_after_resize)
            clip_img = image_utils.resize(clip_img, orig_crop_shape[:2])
            clip_mask = image_utils.resize(clip_mask, orig_crop_shape[:2], interpolation=cv2.INTER_NEAREST)
            t, l, b, r = orig_clip_box
            blend_mask = mask_utils.create_blend_mask(clip_mask)
            blended_img = (
                frame[t:b + 1, l:r + 1, :] * (1 - blend_mask[..., None]) +
                clip_img * (blend_mask[..., None])
            ).clip(0, 255).astype(np.uint8)
            frame[t:b + 1, l:r + 1, :] = blended_img
    
    def _collect_garbage(self, clip_buffer):
        """å‡¦ç†æ¸ˆã¿ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å‰Šé™¤"""
        processed_clips = list(filter(lambda _clip: len(_clip) == 0, clip_buffer))
        for processed_clip in processed_clips:
            clip_buffer.remove(processed_clip)
    
    def _contains_at_least_one_clip_starting_after_frame_num(self, frame_num, clip_buffer):
        """æŒ‡å®šãƒ•ãƒ¬ãƒ¼ãƒ å¾Œã«é–‹å§‹ã™ã‚‹ã‚¯ãƒªãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã‹"""
        return len(clip_buffer) > 0 and frame_num < max(clip_buffer, key=lambda c: c.frame_start).frame_start
    
    def stop(self):
        """åœæ­¢å‡¦ç†"""
        logger.debug("UltraOptimizedFrameRestorer: åœæ­¢é–‹å§‹...")
        start_time = time.time()
        
        self.stop_requested = True
        self.clip_restoration_threads_should_be_running = False
        self.frame_restoration_thread_should_be_running = False
        self.clip_ordering_thread_should_be_running = False
        
        # ãƒ¢ã‚¶ã‚¤ã‚¯æ¤œå‡ºåœæ­¢
        self.mosaic_detector.stop()
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢
        for _ in range(self.parallel_clips):
            threading_utils.put_closing_queue_marker(self.mosaic_clip_queue, "mosaic_clip_queue")
        
        for thread in self.clip_restoration_threads:
            if thread:
                thread.join(timeout=2.0)
        
        if self.clip_ordering_thread:
            self.clip_ordering_thread.join(timeout=2.0)
        
        if self.frame_restoration_thread:
            self.frame_restoration_thread.join(timeout=2.0)
        
        # ãƒãƒƒãƒExecutoråœæ­¢
        if self.batch_executor:
            self.batch_executor.stop()
        
        # GPUè§£æ”¾
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ã‚­ãƒ¥ãƒ¼ã‚¯ãƒªã‚¢
        threading_utils.empty_out_queue(self.mosaic_clip_queue, "mosaic_clip_queue")
        threading_utils.empty_out_queue(self.restored_clip_queue, "restored_clip_queue")
        threading_utils.empty_out_queue(self.frame_detection_queue, "frame_detection_queue")
        threading_utils.empty_out_queue(self.frame_restoration_queue, "frame_restoration_queue")
        threading_utils.empty_out_queue(self.unordered_clips_queue, "unordered_clips_queue")
        
        # çµ±è¨ˆå‡ºåŠ›
        elapsed = time.time() - start_time
        logger.info(f"âœ“ åœæ­¢å®Œäº† ({elapsed:.2f}s)")
        logger.info(f"ğŸ“ˆ å‡¦ç†çµ±è¨ˆ:")
        logger.info(f"   ç·ã‚¯ãƒªãƒƒãƒ—å‡¦ç†æ•°: {self.queue_stats['parallel_clips_processed']}")
        logger.info(f"   ç·GPUæ™‚é–“: {self.queue_stats['total_gpu_time']:.2f}s")
        logger.info(f"   ç·å¾…æ©Ÿæ™‚é–“: {self.queue_stats['total_wait_time']:.2f}s")
        logger.info(f"   ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.queue_stats['clip_timeout_count']}")
        logger.info(f"   ã‚¹ã‚­ãƒƒãƒ—: {self.queue_stats['clip_skipped_count']}")
        
        if self.batch_executor:
            logger.info(f"   {self.batch_executor.get_stats()}")
        
        logger.info(f"   {self.tensor_pool.get_stats()}")
        
        orderer_stats = self.clip_orderer.get_stats()
        logger.info(f"   Ring Bufferä½¿ç”¨ç‡: {orderer_stats['capacity_usage']:.1f}%")
    
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.eof and self.frame_restoration_queue.empty():
            raise StopIteration
        while not self.stop_requested:
            elem = self.frame_restoration_queue.get()
            if self.stop_requested:
                return None
            if elem is None and not self.stop_requested:
                raise StopIteration
            return elem
    
    def get_frame_restoration_queue(self):
        return self.frame_restoration_queue


# å¾Œæ–¹äº’æ›æ€§ã‚¨ã‚¤ãƒªã‚¢ã‚¹
HighlyOptimizedFrameRestorer = UltraOptimizedFrameRestorer
OptimizedFrameRestorer = UltraOptimizedFrameRestorer  # ladaEZ.pyã‹ã‚‰ã®å‘¼ã³å‡ºã—ç”¨


if __name__ == "__main__":
    print("""
    ====================================
    ULTRA-OPTIMIZED Frame Restorer
    ====================================
    
    ã€ä¸»è¦æ”¹å–„ã€‘
    1. ãƒãƒƒãƒGPUå‡¦ç† - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ2-3å€
    2. Ring Bufferé †åºç®¡ç† - O(1)æ“ä½œ
    3. CUDAã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—åŒ– - çœŸã®ä¸¦åˆ—GPUå‡¦ç†
    4. Tensorãƒ—ãƒ¼ãƒ« - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
    5. è¨ºæ–­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›
    
    ã€ä½¿ç”¨æ–¹æ³•ã€‘
    æ—¢å­˜ã®frame_restorer.pyã¨ç½®ãæ›ãˆã‚‹ã ã‘
    
    ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã€‘
    - parallel_clips: ä¸¦åˆ—åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ï¼‰
    - enable_batch_gpu: ãƒãƒƒãƒå‡¦ç†æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueï¼‰
    - batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4ã€ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
    
    ã€æœŸå¾…åŠ¹æœã€‘
    - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 2-3å€å‘ä¸Š
    - GPUåˆ©ç”¨ç‡: 85%ä»¥ä¸Š
    - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 50-70%å‰Šæ¸›
    """)
